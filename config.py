#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Enhanced Configuration module for AI-Powered Naver Cafe Search System
====================================================================
Central configuration and logging setup with AI features
"""

import os
import json
import logging
from pathlib import Path
from typing import Dict, Any

# --- Paths ---
BASE_DIR = Path(__file__).parent
CONFIG_FILE = BASE_DIR / "config.json"
LOG_DIR = BASE_DIR / "logs"

# --- Enhanced Default Configuration ---
DEFAULT_CONFIG = {
    "naver_api": {
        "client_id": os.environ.get("NAVER_CLIENT_ID", ""),
        "client_secret": os.environ.get("NAVER_CLIENT_SECRET", "")
    },
    "search": {
        "max_results_per_keyword": 500,
        "concurrent_requests": 5,
        "request_timeout": 30,
        "cache_ttl": 300,
        "min_relevance_score": 40
    },
    "ai": {
        "semantic_search": True,
        "hidden_gems": True,
        "diversity_mode": True,
        "use_gpu": False,
        "max_semantic_candidates": 1000,
        "diversity_lambda": 0.7,
        "novelty_boost": 1.2,
        "cache_embeddings": True,
        "batch_size": 10
    },
    "nlp": {
        "use_kiwipiepy": True,
        "extract_compounds": True,
        "min_noun_length": 2,
        "sentence_transformer_model": "jhgan/ko-sroberta-multitask",
        "fallback_to_tfidf": True,
        "cache_analysis_results": True
    },
    "server": {
        "host": "127.0.0.1",
        "port": 5000,
        "debug": False,
        "enable_health_check": True,
        "cors_origins": ["*"]
    },
    "performance": {
        "enable_query_cache": True,
        "enable_result_cache": True,
        "max_cache_size": 1000,
        "cache_cleanup_interval": 3600,
        "enable_profiling": False
    },
    "ui": {
        "enable_templates": True,
        "default_view": "card",
        "results_per_page": 50,
        "enable_export": True,
        "theme": "modern"
    }
}

# --- Enhanced Negative Keywords with AI-optimized weights ---
NEGATIVE_KEYWORDS = {
    # Commercial/Sales (High penalty)
    'ì¤‘ê³ ': -50, 'íŒë§¤': -30, 'êµ¬ë§¤': -30, 'ì‚½ë‹ˆë‹¤': -40, 
    'íŒë‹ˆë‹¤': -40, 'ë¶„ì–‘': -60, 'ê±°ëž˜': -35, 'ì–‘ë„': -35, 'ë§¤ë§¤': -35,
    
    # Job-related (Medium penalty - might be relevant for some searches)
    'êµ¬ì¸': -25, 'êµ¬ì§': -25, 'ì•Œë°”': -25, 'ì±„ìš©': -20,
    
    # Spam indicators (High penalty)
    'ê´‘ê³ ': -45, 'í™ë³´': -40, 'ë§ˆì¼€íŒ…': -30, 'í”„ë¡œëª¨ì…˜': -35,
    
    # Low quality indicators (Medium penalty)
    'ê¸‰ë§¤': -30, 'ë–¨ì´': -25, 'ì‹¸ê²Œ': -20, 'ë¤í•‘': -40
}

# --- Enhanced Category Keywords with AI-optimized patterns ---
CATEGORY_KEYWORDS = {
    "ê°œë°œ/IT": [
        'ê°œë°œ', 'í”„ë¡œê·¸ëž˜ë°', 'ì½”ë”©', 'IT', 'it', 'ê°œë°œìž', 'í”„ë¡œê·¸ëž˜ë¨¸',
        'python', 'java', 'javascript', 'react', 'vue', 'angular',
        'ë°±ì—”ë“œ', 'í”„ë¡ íŠ¸ì—”ë“œ', 'í’€ìŠ¤íƒ', 'ai', 'ë¨¸ì‹ ëŸ¬ë‹', 'ë°ì´í„°'
    ],
    "ìŠ¤í„°ë””/êµìœ¡": [
        'ìŠ¤í„°ë””', 'í•™ìŠµ', 'ê³µë¶€', 'êµìœ¡', 'ê°•ì˜', 'í•™ì›', 'ê³¼ì™¸',
        'ìžê²©ì¦', 'ì‹œí—˜', 'í† ìµ', 'í† í”Œ', 'ê³µë¬´ì›', 'ê³µì‹œ', 'ê³ ì‹œ',
        'ì·¨ì—…', 'ë©´ì ‘', 'ìžì†Œì„œ', 'í¬íŠ¸í´ë¦¬ì˜¤'
    ],
    "ì°½ì—…/ë¹„ì¦ˆë‹ˆìŠ¤": [
        'ì°½ì—…', 'ìŠ¤íƒ€íŠ¸ì—…', 'ì‚¬ì—…', 'ë¹„ì¦ˆë‹ˆìŠ¤', 'ê¸°ì—…', 'íšŒì‚¬',
        'ë§ˆì¼€íŒ…', 'ê´‘ê³ ', 'ë¸Œëžœë”©', 'ì„¸ì¼ì¦ˆ', 'ì˜ì—…', 'íˆ¬ìž', 'ìž¬í…Œí¬',
        'ì£¼ì‹', 'íŽ€ë“œ', 'ë¶€ë™ì‚°', 'ê²½ì˜', 'ì „ëžµ', 'ì»¨ì„¤íŒ…'
    ],
    "ì·¨ë¯¸/ë¬¸í™”": [
        'ì·¨ë¯¸', 'ë™í˜¸íšŒ', 'ë¬¸í™”', 'ì˜ˆìˆ ', 'ìŒì•…', 'ì‚¬ì§„', 'ì˜í™”', 'ë“œë¼ë§ˆ',
        'ë…ì„œ', 'ì±…', 'ì„œí‰', 'ìš”ë¦¬', 'ë ˆì‹œí”¼', 'ë§›ì§‘', 'ì¹´íŽ˜', 'ì—¬í–‰',
        'ìº í•‘', 'ë“±ì‚°', 'ìš´ë™', 'í—¬ìŠ¤', 'ìš”ê°€', 'í•„ë¼í…ŒìŠ¤'
    ],
    "ì§€ì—­/ëª¨ìž„": [
        'ì§€ì—­', 'ëª¨ìž„', 'ë™í˜¸íšŒ', 'í´ëŸ½', 'ì»¤ë®¤ë‹ˆí‹°', 'ì¹œëª©', 'ì†Œëª¨ìž„',
        'ë²ˆê°œ', 'ì •ëª¨', 'ì˜¤í”„ë¼ì¸', 'ë§Œë‚¨', 'ë„¤íŠ¸ì›Œí‚¹', 'êµë¥˜',
        'ê°•ë‚¨', 'í™ëŒ€', 'ì‹ ì´Œ', 'ë¶„ë‹¹', 'ì¼ì‚°', 'ìˆ˜ì›', 'ì¸ì²œ'
    ],
    "ìœ¡ì•„/ê°€ì¡±": [
        'ìœ¡ì•„', 'ë§˜ì¹´íŽ˜', 'ìœ¡ì•„ë§˜', 'ì›Œí‚¹ë§˜', 'ì „ì—…ë§˜', 'ì•„ì´', 'ìœ ì•„',
        'ì‹ ìƒì•„', 'ìž„ì‹ ', 'ì¶œì‚°', 'ìœ¡ì•„ìš©í’ˆ', 'ìœ ëª¨ì°¨', 'ë¶„ìœ ',
        'ê°€ì¡±', 'ë¶€ëª¨', 'ì—„ë§ˆ', 'ì•„ë¹ ', 'í˜•ì œ', 'ìžë§¤'
    ],
    "ì—¬í–‰/ê´€ê´‘": [
        'ì—¬í–‰', 'ê´€ê´‘', 'ì—¬í–‰ì§€', 'êµ­ë‚´ì—¬í–‰', 'í•´ì™¸ì—¬í–‰', 'ìžìœ ì—¬í–‰',
        'íŒ¨í‚¤ì§€', 'íŽœì…˜', 'ë¯¼ë°•', 'ê²ŒìŠ¤íŠ¸í•˜ìš°ìŠ¤', 'í˜¸í…”', 'ë¦¬ì¡°íŠ¸',
        'ìº í•‘', 'ë°±íŒ¨í‚¹', 'íŠ¸ë ˆí‚¹', 'ížë§', 'íœ´ì–‘', 'ê´€ê´‘ì§€'
    ],
    "ë¼ì´í”„ìŠ¤íƒ€ì¼": [
        'ë¼ì´í”„ìŠ¤íƒ€ì¼', 'ì¼ìƒ', 'ì†Œí™•í–‰', 'ížë§', 'ì›°ë¹™', 'ê±´ê°•',
        'ë‹¤ì´ì–´íŠ¸', 'ìš´ë™', 'ë·°í‹°', 'íŒ¨ì…˜', 'ì¸í…Œë¦¬ì–´', 'í™ˆì¹´íŽ˜',
        'ë¯¸ë‹ˆë©€', 'ì •ë¦¬ì •ëˆ', 'ì‚´ë¦¼', 'ìžì·¨', 'ë…ë¦½', 'ì…€í”„'
    ]
}

# --- AI Model Configurations ---
AI_MODEL_CONFIGS = {
    "sentence_transformers": {
        "models": {
            "korean_general": "jhgan/ko-sroberta-multitask",
            "korean_sentiment": "beomi/KcELECTRA-base-v2022",
            "multilingual": "paraphrase-multilingual-MiniLM-L12-v2"
        },
        "default_model": "korean_general",
        "device_preference": ["cuda", "mps", "cpu"],
        "max_seq_length": 512,
        "batch_size": 32
    },
    "clustering": {
        "default_clusters": 5,
        "max_clusters": 10,
        "min_cluster_size": 3,
        "algorithm": "kmeans"
    },
    "similarity": {
        "threshold_high": 0.8,
        "threshold_medium": 0.6,
        "threshold_low": 0.4,
        "method": "cosine"
    }
}

# --- Search Templates Configuration ---
SEARCH_TEMPLATES = {
    "study": {
        "programming": {
            "title": "ðŸ’» í”„ë¡œê·¸ëž˜ë° ìŠ¤í„°ë””",
            "keywords": ["í”„ë¡œê·¸ëž˜ë°", "ê°œë°œ", "ì½”ë”©"],
            "description": "ê°œë°œìžë“¤ì´ ëª¨ì—¬ ê¸°ìˆ ì„ ê³µìœ í•˜ê³  í•¨ê»˜ ì„±ìž¥í•˜ëŠ” ì¹´íŽ˜",
            "ai_boost": 1.2
        },
        "english": {
            "title": "ðŸŒ ì˜ì–´ í•™ìŠµ",
            "keywords": ["ì˜ì–´", "í† ìµ", "í† í”Œ"],
            "description": "ì˜ì–´ ì‹¤ë ¥ í–¥ìƒì„ ìœ„í•œ ìŠ¤í„°ë”” ê·¸ë£¹",
            "ai_boost": 1.1
        },
        "certification": {
            "title": "ðŸ“œ ìžê²©ì¦ ìŠ¤í„°ë””",
            "keywords": ["ìžê²©ì¦", "ì‹œí—˜", "ìžì†Œì„œ"],
            "description": "ë‹¤ì–‘í•œ ìžê²©ì¦ ì·¨ë“ì„ ìœ„í•œ ìŠ¤í„°ë””",
            "ai_boost": 1.0
        }
    },
    "business": {
        "startup": {
            "title": "ðŸš€ ì°½ì—… ì»¤ë®¤ë‹ˆí‹°",
            "keywords": ["ì°½ì—…", "ìŠ¤íƒ€íŠ¸ì—…", "ì‚¬ì—…"],
            "description": "ì°½ì—…ê°€ë“¤ì´ ê²½í—˜ì„ ë‚˜ëˆ„ê³  ë„¤íŠ¸ì›Œí‚¹í•˜ëŠ” ê³µê°„",
            "ai_boost": 1.3
        },
        "freelancer": {
            "title": "ðŸ’¼ í”„ë¦¬ëžœì„œ",
            "keywords": ["í”„ë¦¬ëžœì„œ", "ë¶€ì—…", "ì‚¬ì´ë“œ"],
            "description": "í”„ë¦¬ëžœì„œë“¤ì˜ ì •ë³´ ê³µìœ ì™€ í”„ë¡œì íŠ¸ í˜‘ì—…",
            "ai_boost": 1.2
        }
    }
}

def load_config() -> Dict[str, Any]:
    """
    Load configuration from file or use defaults with AI enhancements
    
    Returns:
        Dict containing configuration settings
    """
    if CONFIG_FILE.exists():
        try:
            with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
                user_config = json.load(f)
                # Deep merge with defaults
                config = _deep_merge_config(DEFAULT_CONFIG.copy(), user_config)
                
                # Validate AI configuration
                config = _validate_ai_config(config)
                
                return config
        except Exception as e:
            logging.warning(f"Failed to load config file: {e}. Using defaults.")
    
    return DEFAULT_CONFIG.copy()

def _deep_merge_config(default: Dict, user: Dict) -> Dict:
    """Deep merge user config with defaults"""
    result = default.copy()
    
    for key, value in user.items():
        if key in result and isinstance(result[key], dict) and isinstance(value, dict):
            result[key] = _deep_merge_config(result[key], value)
        else:
            result[key] = value
    
    return result

def _validate_ai_config(config: Dict[str, Any]) -> Dict[str, Any]:
    """Validate and adjust AI configuration based on system capabilities"""
    ai_config = config.get('ai', {})
    
    # Check GPU availability
    if ai_config.get('use_gpu', False):
        try:
            import torch
            if not torch.cuda.is_available():
                logging.warning("GPU requested but CUDA not available. Falling back to CPU.")
                ai_config['use_gpu'] = False
        except ImportError:
            logging.warning("PyTorch not available. Disabling GPU usage.")
            ai_config['use_gpu'] = False
    
    # Adjust batch size based on memory
    try:
        import psutil
        memory_gb = psutil.virtual_memory().total / (1024**3)
        
        if memory_gb < 8:
            ai_config['batch_size'] = min(ai_config.get('batch_size', 10), 5)
            ai_config['max_semantic_candidates'] = min(ai_config.get('max_semantic_candidates', 1000), 500)
            logging.info("Limited memory detected. Reducing AI batch sizes.")
        elif memory_gb >= 16:
            ai_config['batch_size'] = max(ai_config.get('batch_size', 10), 20)
            logging.info("Ample memory detected. Increasing AI batch sizes.")
    except ImportError:
        pass
    
    config['ai'] = ai_config
    return config

def save_config(config: Dict[str, Any]):
    """Save configuration to file with pretty formatting"""
    try:
        # Remove sensitive information before saving
        config_to_save = config.copy()
        if 'naver_api' in config_to_save:
            api_config = config_to_save['naver_api'].copy()
            if api_config.get('client_secret'):
                api_config['client_secret'] = "***HIDDEN***"
            config_to_save['naver_api'] = api_config
        
        with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
            json.dump(config_to_save, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logging.error(f"Failed to save config: {e}")

def setup_logging(level: str = 'INFO'):
    """
    Setup enhanced logging for the AI application
    """
    # Create logs directory
    LOG_DIR.mkdir(exist_ok=True)
    
    # Configure root logger with enhanced format
    log_format = '%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'
    
    logging.basicConfig(
        level=getattr(logging, level.upper()),
        format=log_format,
        handlers=[
            logging.FileHandler(LOG_DIR / 'app.log', encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    
    # Create separate loggers for different components
    ai_logger = logging.getLogger('ai')
    ai_handler = logging.FileHandler(LOG_DIR / 'ai.log', encoding='utf-8')
    ai_handler.setFormatter(logging.Formatter(log_format))
    ai_logger.addHandler(ai_handler)
    ai_logger.setLevel(logging.DEBUG)
    
    performance_logger = logging.getLogger('performance')
    perf_handler = logging.FileHandler(LOG_DIR / 'performance.log', encoding='utf-8')
    perf_handler.setFormatter(logging.Formatter(log_format))
    performance_logger.addHandler(perf_handler)
    
    # Reduce noise from external libraries
    logging.getLogger('aiohttp').setLevel(logging.WARNING)
    logging.getLogger('asyncio').setLevel(logging.WARNING)
    logging.getLogger('urllib3').setLevel(logging.WARNING)
    logging.getLogger('transformers').setLevel(logging.WARNING)
    logging.getLogger('sentence_transformers').setLevel(logging.WARNING)

def get_ai_model_config(model_type: str = 'sentence_transformers') -> Dict[str, Any]:
    """Get AI model configuration"""
    return AI_MODEL_CONFIGS.get(model_type, {})

def get_search_templates() -> Dict[str, Any]:
    """Get search templates configuration"""
    return SEARCH_TEMPLATES

def get_device_config() -> str:
    """Determine the best device for AI models"""
    config = load_config()
    
    if not config.get('ai', {}).get('use_gpu', False):
        return 'cpu'
    
    try:
        import torch
        if torch.cuda.is_available():
            return 'cuda'
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            return 'mps'  # Apple Silicon
    except ImportError:
        pass
    
    return 'cpu'

# Load configuration on module import
CONFIG = load_config()

# Setup logging
setup_logging(CONFIG.get('logging', {}).get('level', 'INFO'))

# Create logger for this module
logger = logging.getLogger(__name__)

# Log configuration info
logger.info(f"Configuration loaded: AI enabled={CONFIG.get('ai', {}).get('semantic_search', False)}")
logger.info(f"Device configuration: {get_device_config()}")